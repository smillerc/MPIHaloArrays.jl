var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API","title":"API Reference","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [MPIHaloArrays]\nOrder   = [:type, :function]","category":"page"},{"location":"api/#MPIHaloArrays.AbstractParallelTopology","page":"API","title":"MPIHaloArrays.AbstractParallelTopology","text":"An abstract AbstractParallelTopology type that is extended by either a CartesianTopology or GraphTopology (future)\n\n\n\n\n\n","category":"type"},{"location":"api/#MPIHaloArrays.CartesianTopology","page":"API","title":"MPIHaloArrays.CartesianTopology","text":"CartesianTopology\n\nThe CartesianTopology type holds neighbor information, current rank, etc.\n\nFields\n\ncomm: MPI commicator object\nnprocs: Number of total processors (global)\nrank: Current rank\ncoords: Coordinates in the global space, i.e. (0,1,1)\nglobal_dims: Dimensions of the global domain, i.e. (4,4) is a 4x4 global domain\nisperiodic: Vector{Bool}; Perodicity of each dimension, i.e. (false, true, true) means y and z are periodic\nneighbors: OffsetArray{Int}; Neighbor ranks (including corners), indexed as [[ilo, center, ihi], i, j, k]\n\n\n\n\n\n","category":"type"},{"location":"api/#MPIHaloArrays.CartesianTopology-Tuple{MPI.Comm, Vector{Bool}}","page":"API","title":"MPIHaloArrays.CartesianTopology","text":"Create CartesianTopology only with the vector of boundary periodicity given.  This finds the optimal sub-domain ordering for the user.\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.CartesianTopology-Tuple{MPI.Comm, Vector{Int64}, Vector{Bool}}","page":"API","title":"MPIHaloArrays.CartesianTopology","text":"Create a CartesianTopology type that holds neighbor information, current rank, etc.\n\nArguments\n\ndims: Dimensions of the domain in each direction, e.g. [4,3] means a total of 12 procs, with 4 in x and 3 in y\nperiodicity: Vector of bools to set if the domain is periodic along a specific dimension\n\nExample\n\n\n# Create a topology of 4x4 with periodic boundaries in both directions\nP = CartesianTopology([4,4], [true, true])\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.MPIHaloArray","page":"API","title":"MPIHaloArrays.MPIHaloArray","text":"MPIHaloArray\n\nFields\n\ndata: AbstractArray{T,N} - contains the local data on the current rank\npartitioning: partitioning datatype\ncomm: MPI communicator\nwindow: MPI window\nneighbor_ranks : Vector{Int} - IDs of the neighboring arrays/MPI procs\ncoords : Vector{Int} - Coordinates in the global MPI space\nrank: Current MPI rank\n\n\n\n\n\n","category":"type"},{"location":"api/#MPIHaloArrays.MPIHaloArray-Union{Tuple{N}, Tuple{T}, Tuple{AbstractArray{T, N}, CartesianTopology, Int64}} where {T, N}","page":"API","title":"MPIHaloArrays.MPIHaloArray","text":"MPIHaloArray constructor\n\nArguments\n\nA: AbstractArray{T,N}\ntopo: Parallel topology type, e.g. CartesianTopology\nnhalo: Number of halo cells\n\nKeyword Arguments\n\ndo_corners: [true] Exchange corner halo regions \ncom_model: [p2p] Communication model, e.g. :p2p is point-to-point (Isend, Irecv), :rma is onesided (Get,Put), :shared is MPI's shared memory model\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.coord_to_rank-NTuple{4, Any}","page":"API","title":"MPIHaloArrays.coord_to_rank","text":"Helper function to find rank based on coordinates\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.denominators1-Tuple{Integer}","page":"API","title":"MPIHaloArrays.denominators1","text":"Return all common denominators of n\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.gatherglobal-Union{Tuple{MPIHaloArray{T, 1}}, Tuple{T}} where T","page":"API","title":"MPIHaloArrays.gatherglobal","text":"Gather all MPIHaloArrays onto the root MPI rank and stitch together. This will ignore halo region data and create a Array that represents the global state.\n\nArguments\n\nA: MPIHaloArray\nroot: MPI rank to gather A to\nhalo_dims: Tuple of the dimensions that halo exchanges occur on (not fully working yet)\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.get_dims-Union{Tuple{T}, Tuple{Array{Tuple{T, T, T}, 1}, Any}} where T<:Integer","page":"API","title":"MPIHaloArrays.get_dims","text":"Get the dimensions of each chunk\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.get_subdomain_dimension_sizes-Tuple{Any, Any, Any}","page":"API","title":"MPIHaloArrays.get_subdomain_dimension_sizes","text":"get_subdomain_dimension_sizes(A, tile_dims, A_halo_dims)\n\nGet the size along each dimension in (i,j,k) of the subdomain, based on a given array A. The tile_dims is the shape of the global domain, e.g. (4,2) means 4 tiles or subdomains in i and 2 in j. A_halo_dims is the tuple of which dimensions the halo exchanges take place on, e.g. (2,3).\n\nExample\n\nA = rand(4,200,100); dims=(2,3), tile_dims=(4,2)\nget_subdomain_dimension_sizes(A, tile_dims, dims) # [[i][j]] --> [[50,50,50,50],[100,100]]\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.globalsize-Union{Tuple{T}, Tuple{Array{Tuple{T, T, T}, 1}, Any}} where T<:Integer","page":"API","title":"MPIHaloArrays.globalsize","text":"Find the global dims of based on the list of local MPIHaloArray sizes\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.hi_indices-Tuple{Any, Any, Any}","page":"API","title":"MPIHaloArrays.hi_indices","text":"Helper functions to get the high side halo and domain starting/ending indices\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.ihi_neighbor-Tuple{CartesianTopology}","page":"API","title":"MPIHaloArrays.ihi_neighbor","text":"Neighbor rank in the i+1 direction\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.ilo_neighbor-Tuple{CartesianTopology}","page":"API","title":"MPIHaloArrays.ilo_neighbor","text":"Neighbor rank in the i-1 direction\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.jhi_neighbor-Tuple{CartesianTopology}","page":"API","title":"MPIHaloArrays.jhi_neighbor","text":"Neighbor rank in the j+1 direction\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.jlo_neighbor-Tuple{CartesianTopology}","page":"API","title":"MPIHaloArrays.jlo_neighbor","text":"Neighbor rank in the j-1 direction\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.khi_neighbor-Tuple{CartesianTopology}","page":"API","title":"MPIHaloArrays.khi_neighbor","text":"Neighbor rank in the k+1 direction\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.klo_neighbor-Tuple{CartesianTopology}","page":"API","title":"MPIHaloArrays.klo_neighbor","text":"Neighbor rank in the k-1 direction\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.lo_indices-Tuple{Any, Any, Any}","page":"API","title":"MPIHaloArrays.lo_indices","text":"Helper functions to get the low side halo and domain starting/ending indices\n\nArguments\n\nfield: Array \ndim: Dimension to check indices on\nnhalo: Number of halo entries\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.match_tile_halo_dim_sizes-Tuple{Any, Any}","page":"API","title":"MPIHaloArrays.match_tile_halo_dim_sizes","text":"match_tile_halo_dim_sizes(tile_dims, halo_dims)\n\nEnsure that the tile dimension tuple is the same length as the halo dim tuple. If not, then pad with ones.\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.neighbor-Tuple{CartesianTopology, Int64, Int64, Int64}","page":"API","title":"MPIHaloArrays.neighbor","text":"neighbor(p::CartesianTopology, i_offset::Int, j_offset::Int, k_offset::Int)\n\nFind the neighbor rank based on the offesets in (i,j,k). This follows the traditional array index convention rather than MPI's version, so an i_offset=1 will shift up in the array indexing.\n\nArguments\n\np : CartesianTopology type\ni_offset: Offset in the i direction\nj_offset: Offset in the j direction\nk_offset: Offset in the k direction\n\nExample:\n\n# Makes a 4x4 domain with periodic boundaries in both dimensions\nP = CartesianTopology([4,4], [true, true])\n\n# Find the ihi neighbor\nihi = neighbor(P,+1,0,0)\n\n# Find the upper ihi corner neighbor (ihi and jhi side)\nihijhi_corner = neighbor(P,+1,+1,0)\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.num_2d_tiles1-Tuple{Any}","page":"API","title":"MPIHaloArrays.num_2d_tiles1","text":"Returns the optimal number of tiles in (i,j) given total number of tiles n\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.num_3d_tiles1-Tuple{Any}","page":"API","title":"MPIHaloArrays.num_3d_tiles1","text":"Returns the optimal number of tiles in (i,j,k) given total number of tiles n\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.offset_coord_to_rank-Tuple{Any, Any, Any, Int64, Int64, Int64}","page":"API","title":"MPIHaloArrays.offset_coord_to_rank","text":"Helper function to find rank based on 3D offsets\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.offset_coord_to_rank-Tuple{Any, Any, Any, Int64, Int64}","page":"API","title":"MPIHaloArrays.offset_coord_to_rank","text":"Helper function to find rank based on 2D offsets\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.scatterglobal-Union{Tuple{T}, Tuple{AbstractVector{T}, Int64, Int64, AbstractParallelTopology}} where T","page":"API","title":"MPIHaloArrays.scatterglobal","text":"Partition the array A on the rank root into chunks based on the given parallel toplogy. The array data in A does not have halo regions. The MPIHaloArray constructor adds the halo regions. This returns a MPIHaloArray\n\nArguments\n\nA: Global array to be split up into chunks and sent to all ranks. This does not include halo cells\nroot: MPI rank that A lives on\nnhalo: Number of halo cells to create\nhalo_dims: Tuple of the dimensions that halo exchanges occur on (not fully working yet)\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.split_count-Tuple{Integer, Integer}","page":"API","title":"MPIHaloArrays.split_count","text":"split_count(N::Integer, n::Integer)\n\nReturn a vector of n integers which are approximately equally sized and sum to N.\n\n\n\n\n\n","category":"method"},{"location":"api/#MPIHaloArrays.sync_edges_rma!-Union{Tuple{MPIHaloArray{T, 1}}, Tuple{T}} where T","page":"API","title":"MPIHaloArrays.sync_edges_rma!","text":"Sync the edges of the array A with it's neighbors\n\n\n\n\n\n","category":"method"},{"location":"mpihaloarray/#MPIHaloArray","page":"MPIHaloArray","title":"MPIHaloArray","text":"","category":"section"},{"location":"mpihaloarray/","page":"MPIHaloArray","title":"MPIHaloArray","text":"This is an example","category":"page"},{"location":"mpihaloarray/#Halo-Exchange","page":"MPIHaloArray","title":"Halo Exchange","text":"","category":"section"},{"location":"mpihaloarray/","page":"MPIHaloArray","title":"MPIHaloArray","text":"(Image: 1D halo exchange)","category":"page"},{"location":"mpihaloarray/","page":"MPIHaloArray","title":"MPIHaloArray","text":"MPIHaloArrays.MPIHaloArray\nMPIHaloArrays.fillhalo!\nMPIHaloArrays.filldomain!","category":"page"},{"location":"topology/#AbstractParallelTopology","page":"AbstractParallelTopology","title":"AbstractParallelTopology","text":"","category":"section"},{"location":"topology/","page":"AbstractParallelTopology","title":"AbstractParallelTopology","text":"Parallel topology sets up the neighbors","category":"page"},{"location":"topology/","page":"AbstractParallelTopology","title":"AbstractParallelTopology","text":"MPIHaloArrays.AbstractParallelTopology\nMPIHaloArrays.CartesianTopology","category":"page"},{"location":"examples/02-halo2d/#D-Halo-Example","page":"2D Halo Example","title":"2D Halo Example","text":"","category":"section"},{"location":"examples/02-halo2d/","page":"2D Halo Example","title":"2D Halo Example","text":"# examples/02-halo2d.jl\nusing MPI, MPIHaloArrays\n\nMPI.Init()\nconst comm = MPI.COMM_WORLD\nconst rank = MPI.Comm_rank(comm)\nconst nprocs = MPI.Comm_size(comm)\n\n@assert nprocs == 8 \"This example is designed with 8 processes...\"\n\n\nfunction print_array(U, proc)\n    if rank == proc\n        println(\"rank: \", proc)\n        display(U.data)\n        println()\n    end\n    MPI.Barrier(comm)\nend\n\ntopology = CartesianTopology(comm, [4,2], [false, false])\n\nnhalo = 2\nni = 6\nnj = 5\n\nA = MPIHaloArray(zeros(Int, ni, nj), topology, nhalo)\nfillhalo!(A, -1)\nfilldomain!(A, rank)\n\nif rank == 0 println(\"Before sync\") end\nfor p in 0:nprocs-1\n    print_array(A, p)\nend\n\nupdatehalo!(A)\n\nif rank == 0 println(\"After sync\") end\nfor p in 0:nprocs-1\n    print_array(A, p)\nend\n\nGC.gc()\nMPI.Finalize()","category":"page"},{"location":"examples/02-halo2d/","page":"2D Halo Example","title":"2D Halo Example","text":"> mpiexecjl -n 8 julia examples/02-halo2d.jl\nBefore sync\nrank: 0\n10×9 Matrix{Int64}:\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1   0   0   0   0   0  -1  -1\n -1  -1   0   0   0   0   0  -1  -1\n -1  -1   0   0   0   0   0  -1  -1\n -1  -1   0   0   0   0   0  -1  -1\n -1  -1   0   0   0   0   0  -1  -1\n -1  -1   0   0   0   0   0  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\nrank: 1\n10×9 Matrix{Int64}:\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1   1   1   1   1   1  -1  -1\n -1  -1   1   1   1   1   1  -1  -1\n -1  -1   1   1   1   1   1  -1  -1\n -1  -1   1   1   1   1   1  -1  -1\n -1  -1   1   1   1   1   1  -1  -1\n -1  -1   1   1   1   1   1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\nrank: 2\n10×9 Matrix{Int64}:\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1   2   2   2   2   2  -1  -1\n -1  -1   2   2   2   2   2  -1  -1\n -1  -1   2   2   2   2   2  -1  -1\n -1  -1   2   2   2   2   2  -1  -1\n -1  -1   2   2   2   2   2  -1  -1\n -1  -1   2   2   2   2   2  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\nrank: 3\n10×9 Matrix{Int64}:\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1   3   3   3   3   3  -1  -1\n -1  -1   3   3   3   3   3  -1  -1\n -1  -1   3   3   3   3   3  -1  -1\n -1  -1   3   3   3   3   3  -1  -1\n -1  -1   3   3   3   3   3  -1  -1\n -1  -1   3   3   3   3   3  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\nrank: 4\n10×9 Matrix{Int64}:\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1   4   4   4   4   4  -1  -1\n -1  -1   4   4   4   4   4  -1  -1\n -1  -1   4   4   4   4   4  -1  -1\n -1  -1   4   4   4   4   4  -1  -1\n -1  -1   4   4   4   4   4  -1  -1\n -1  -1   4   4   4   4   4  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\nrank: 5\n10×9 Matrix{Int64}:\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1   5   5   5   5   5  -1  -1\n -1  -1   5   5   5   5   5  -1  -1\n -1  -1   5   5   5   5   5  -1  -1\n -1  -1   5   5   5   5   5  -1  -1\n -1  -1   5   5   5   5   5  -1  -1\n -1  -1   5   5   5   5   5  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\nrank: 6\n10×9 Matrix{Int64}:\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1   6   6   6   6   6  -1  -1\n -1  -1   6   6   6   6   6  -1  -1\n -1  -1   6   6   6   6   6  -1  -1\n -1  -1   6   6   6   6   6  -1  -1\n -1  -1   6   6   6   6   6  -1  -1\n -1  -1   6   6   6   6   6  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\nrank: 7\n10×9 Matrix{Int64}:\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1   7   7   7   7   7  -1  -1\n -1  -1   7   7   7   7   7  -1  -1\n -1  -1   7   7   7   7   7  -1  -1\n -1  -1   7   7   7   7   7  -1  -1\n -1  -1   7   7   7   7   7  -1  -1\n -1  -1   7   7   7   7   7  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\nAfter sync\nrank: 0\n10×9 Matrix{Int64}:\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1   0   0   0   0   0   4   4\n -1  -1   0   0   0   0   0   4   4\n -1  -1   0   0   0   0   0   4   4\n -1  -1   0   0   0   0   0   4   4\n -1  -1   0   0   0   0   0   4   4\n -1  -1   0   0   0   0   0   4   4\n -1  -1   1   1   1   1   1   5   5\n -1  -1   1   1   1   1   1   5   5\nrank: 1\n10×9 Matrix{Int64}:\n -1  -1  0  0  0  0  0  4  4\n -1  -1  0  0  0  0  0  4  4\n -1  -1  1  1  1  1  1  5  5\n -1  -1  1  1  1  1  1  5  5\n -1  -1  1  1  1  1  1  5  5\n -1  -1  1  1  1  1  1  5  5\n -1  -1  1  1  1  1  1  5  5\n -1  -1  1  1  1  1  1  5  5\n -1  -1  2  2  2  2  2  6  6\n -1  -1  2  2  2  2  2  6  6\nrank: 2\n10×9 Matrix{Int64}:\n -1  -1  1  1  1  1  1  5  5\n -1  -1  1  1  1  1  1  5  5\n -1  -1  2  2  2  2  2  6  6\n -1  -1  2  2  2  2  2  6  6\n -1  -1  2  2  2  2  2  6  6\n -1  -1  2  2  2  2  2  6  6\n -1  -1  2  2  2  2  2  6  6\n -1  -1  2  2  2  2  2  6  6\n -1  -1  3  3  3  3  3  7  7\n -1  -1  3  3  3  3  3  7  7\nrank: 3\n10×9 Matrix{Int64}:\n -1  -1   2   2   2   2   2   6   6\n -1  -1   2   2   2   2   2   6   6\n -1  -1   3   3   3   3   3   7   7\n -1  -1   3   3   3   3   3   7   7\n -1  -1   3   3   3   3   3   7   7\n -1  -1   3   3   3   3   3   7   7\n -1  -1   3   3   3   3   3   7   7\n -1  -1   3   3   3   3   3   7   7\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\nrank: 4\n10×9 Matrix{Int64}:\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n  0   0   4   4   4   4   4  -1  -1\n  0   0   4   4   4   4   4  -1  -1\n  0   0   4   4   4   4   4  -1  -1\n  0   0   4   4   4   4   4  -1  -1\n  0   0   4   4   4   4   4  -1  -1\n  0   0   4   4   4   4   4  -1  -1\n  1   1   5   5   5   5   5  -1  -1\n  1   1   5   5   5   5   5  -1  -1\nrank: 5\n10×9 Matrix{Int64}:\n 0  0  4  4  4  4  4  -1  -1\n 0  0  4  4  4  4  4  -1  -1\n 1  1  5  5  5  5  5  -1  -1\n 1  1  5  5  5  5  5  -1  -1\n 1  1  5  5  5  5  5  -1  -1\n 1  1  5  5  5  5  5  -1  -1\n 1  1  5  5  5  5  5  -1  -1\n 1  1  5  5  5  5  5  -1  -1\n 2  2  6  6  6  6  6  -1  -1\n 2  2  6  6  6  6  6  -1  -1\nrank: 6\n10×9 Matrix{Int64}:\n 1  1  5  5  5  5  5  -1  -1\n 1  1  5  5  5  5  5  -1  -1\n 2  2  6  6  6  6  6  -1  -1\n 2  2  6  6  6  6  6  -1  -1\n 2  2  6  6  6  6  6  -1  -1\n 2  2  6  6  6  6  6  -1  -1\n 2  2  6  6  6  6  6  -1  -1\n 2  2  6  6  6  6  6  -1  -1\n 3  3  7  7  7  7  7  -1  -1\n 3  3  7  7  7  7  7  -1  -1\nrank: 7\n10×9 Matrix{Int64}:\n  2   2   6   6   6   6   6  -1  -1\n  2   2   6   6   6   6   6  -1  -1\n  3   3   7   7   7   7   7  -1  -1\n  3   3   7   7   7   7   7  -1  -1\n  3   3   7   7   7   7   7  -1  -1\n  3   3   7   7   7   7   7  -1  -1\n  3   3   7   7   7   7   7  -1  -1\n  3   3   7   7   7   7   7  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1\n -1  -1  -1  -1  -1  -1  -1  -1  -1","category":"page"},{"location":"examples/04-diffusion2d/#D-Diffusion","page":"2D Diffusion","title":"2D Diffusion","text":"","category":"section"},{"location":"examples/04-diffusion2d/","page":"2D Diffusion","title":"2D Diffusion","text":"# examples/04-diffusion2d.jl\nusing MPI, MPIHaloArrays\nusing Plots\ngr()\n\nMPI.Init()\nconst comm = MPI.COMM_WORLD\nconst rank = MPI.Comm_rank(comm)\nconst nprocs = MPI.Comm_size(comm)\nconst root = 0 # root rank\n\n\"\"\"Establish the initial conditions, e.g. place a high temp rectangle in the center\"\"\"\nfunction initialize!(x, y, T_0, T_c)\n    T = zeros(length(x), length(y))\n    fill!(T, T_0)\n\n    dx0 = 0.2 # size of the central region\n    dy0 = 0.3 # size of the central region\n    for j in 1:length(y)\n        for i in 1:length(x)\n            if -dx0 < x[i] < dx0 && -dy0 < y[j] < dy0\n                T[i, j] = T_c\n            end\n        end\n    end\n    return T\nend\n\n\"\"\"Perform 2D heat diffusion\"\"\"\nfunction diffusion!(T, T_new, α, dt, dx, dy)\n    ilo, ihi, jlo, jhi = localindices(T)\n    for j in jlo:jhi\n        for i in ilo:ihi\n            T_new[i, j] = T[i, j] + α * dt * ((T[i-1, j] - 2 * T[i, j] + T[i+1, j]) / dx^2 +\n                                              (T[i, j-1] - 2 * T[i, j] + T[i, j+1]) / dy^2)\n        end\n    end\nend\n\nfunction plot_temp(T, iter; root = 0)\n    T_result = gatherglobal(T; root = root)\n    if rank == root\n        println(\"Plotting t$(iter).png\")\n        p1 = contour(T_result, fill = true, color = :viridis, aspect_ratio = :equal)\n        plot(p1)\n        savefig(\"t$(iter).png\")\n    end\nend\n\n# ----------------------------------------------------------------\n# Initial conditions\ndx = 0.01; dy = 0.01; # grid spacing\nα = 0.1 # thermal diffusivity\ndt = dx^2 * dy^2 / (2.0 * α * (dx^2 + dy^2)) # stable time step\n\nx = -1:dx:1 |> collect # x grid \ny = -2:dy:2 |> collect # y grid \n\nT_0 = 100.0 # initial temperature\nT_c = 200.0 # temperature at the center hot region\n\n# Initialize the temperature field\nT_global = initialize!(x, y, T_0, T_c)\n\n# ----------------------------------------------------------------\n# Parallel topology construction\nnhalo = 1 # only a stencil of 5 cells is needed, so 1 halo cell is sufficient\n@assert nprocs == 4 \"This example is designed with 4 processes, \nbut can be changed in the topology construction...\"\ntopology = CartesianTopology(comm, [2,2], [true, true]) # periodic boundary conditions\n\n# ----------------------------------------------------------------\n# Plot initial conditions\nif rank == root\n    println(\"Plotting initial conditions\")\n    p1 = contour(T_global, fill = true, color = :viridis, aspect_ratio = :equal)\n    plot(p1)\n    savefig(\"t0.png\")\nend\n\n# ----------------------------------------------------------------\n# Distribute the work to each process, e.g. domain decomposition\nTⁿ = scatterglobal(T_global, root, nhalo, topology; \n                   do_corners = false) # the 5-cell stencil doesn't use corner info, \n                                       # so this saves communication time\nTⁿ⁺¹ = deepcopy(Tⁿ) # T at the next timestep\n\nniter = 500\nplot_interval = 50\ninfo_interval = 10\n\nplot_temp(Tⁿ, 0) # Plot initial conditions\n\n# Time loop\nfor iter in 1:niter\n    if rank == root && iter % info_interval == 0 println(\"Iteration: $iter\") end\n    if iter % plot_interval == 0 plot_temp(Tⁿ, iter) end\n\n    updatehalo!(Tⁿ)\n    diffusion!(Tⁿ, Tⁿ⁺¹, α, dt, dx, dy)\n    Tⁿ.data .= Tⁿ⁺¹.data # update the next time-step\nend\n\nGC.gc()\nMPI.Finalize()","category":"page"},{"location":"examples/04-diffusion2d/","page":"2D Diffusion","title":"2D Diffusion","text":"> mpiexecjl -n 4 julia examples/04-diffusion2d.jl\nPlotting initial conditions\nPlotting t0.png\nIteration: 10\nIteration: 20\nIteration: 30\nIteration: 40\nIteration: 50\nPlotting t50.png\nIteration: 60\nIteration: 70\nIteration: 80\nIteration: 90\nIteration: 100\nPlotting t100.png\nIteration: 110\nIteration: 120\nIteration: 130\nIteration: 140\nIteration: 150\nPlotting t150.png\nIteration: 160\nIteration: 170\nIteration: 180\nIteration: 190\nIteration: 200\nPlotting t200.png\nIteration: 210\nIteration: 220\nIteration: 230\nIteration: 240\nIteration: 250\nPlotting t250.png\nIteration: 260\nIteration: 270\nIteration: 280\nIteration: 290\nIteration: 300\nPlotting t300.png\nIteration: 310\nIteration: 320\nIteration: 330\nIteration: 340\nIteration: 350\nPlotting t350.png\nIteration: 360\nIteration: 370\nIteration: 380\nIteration: 390\nIteration: 400\nPlotting t400.png\nIteration: 410\nIteration: 420\nIteration: 430\nIteration: 440\nIteration: 450\nPlotting t450.png\nIteration: 460\nIteration: 470\nIteration: 480\nIteration: 490\nIteration: 500\nPlotting t500.png","category":"page"},{"location":"examples/03-halo3d/#D-Halo-Example","page":"3D Halo Example","title":"3D Halo Example","text":"","category":"section"},{"location":"examples/03-halo3d/","page":"3D Halo Example","title":"3D Halo Example","text":"# examples/03-halo3d.jl\nusing MPI, MPIHaloArrays\n\nMPI.Init()\nconst comm = MPI.COMM_WORLD\nconst rank = MPI.Comm_rank(comm)\nconst nprocs = MPI.Comm_size(comm)\n\n@assert nprocs == 8 \"This example is designed with 8 processes...\"\n\ntopology = CartesianTopology(comm, [2,2,2], [false,false,false])\n\nnhalo = 2\nni = 8\nnj = 8\nnk = 8\n\nA = MPIHaloArray(zeros(Int, ni, nj, nk), topology, nhalo; do_corners=true)\nfilldomain!(A, rank)\nfillhalo!(A, -1)\n\nupdatehalo!(A)\n\nGC.gc()\nMPI.Finalize()","category":"page"},{"location":"examples/03-halo3d/","page":"3D Halo Example","title":"3D Halo Example","text":"> mpiexecjl -n 8 julia examples/03-halo3d.jl","category":"page"},{"location":"#MPIHaloArrays","page":"MPIHaloArrays.jl","title":"MPIHaloArrays","text":"","category":"section"},{"location":"","page":"MPIHaloArrays.jl","title":"MPIHaloArrays.jl","text":"MPIHaloArrays is a high-level array type to help with halo, or ghost-cell exchanges commonly found in large-scale PDE problems. Very similar in goals and design to MPIArrays.jl and ImplicitGlobalGrid.jl. Domains can be decomposed into 1, 2, or 3D topology. Currently arrays are limited to 1, 2, or 3D.","category":"page"},{"location":"#Installation","page":"MPIHaloArrays.jl","title":"Installation","text":"","category":"section"},{"location":"","page":"MPIHaloArrays.jl","title":"MPIHaloArrays.jl","text":"The package is registered and can be installed with","category":"page"},{"location":"","page":"MPIHaloArrays.jl","title":"MPIHaloArrays.jl","text":"pkg> add MPIHaloArrays","category":"page"},{"location":"#Basic-Usage","page":"MPIHaloArrays.jl","title":"Basic Usage","text":"","category":"section"},{"location":"","page":"MPIHaloArrays.jl","title":"MPIHaloArrays.jl","text":"This example shows how to set up the initial array, fill the halo/domain cells, and do a halo exchange","category":"page"},{"location":"","page":"MPIHaloArrays.jl","title":"MPIHaloArrays.jl","text":"using MPI, MPIHaloArrays\n\nMPI.Init()\nrank = MPI.Comm_rank(comm)\n\n# Create the MPI topology\ntopo = CartesianTopology([4,4], # use a 4x4 decomposition\n                         [true, true]; # periodic in both dimensions   \n                         do_corners=false) # exchange corner halo regions (significant speed advantage if you don't need it)\n\nnhalo = 2 # Number of halo cells in each dimension (fixed for all dimensions)\nN = 200\n\n# create the array type; this pads the data on all sides with halo regions\nx = MPIHaloArray(rand(N,N), topo, nhalo)\n\n# fill all the halo regions with -1\nfillhalo!(A, -1)\n\n# fill the domain region with the current rank\nfilldomain!(A, rank)\n\n# local (current rank) indexing works just like a normal array\nx[1,1] .= 2.0\n\n# Get the local/global indices of the _domain_ data (not including the halo cells)\nilo, ihi, jlo, jhi = localindices(x) # -> useful for looping without going into halo regions\nilo_g, ihi_g, jlo_g, jhi_g = globalindices(x)\n\n# Exchange data with neighbors\nupdatehalo!(x)\n\nGC.gc()\nMPI.Finalize()","category":"page"},{"location":"","page":"MPIHaloArrays.jl","title":"MPIHaloArrays.jl","text":"Scatter and gather operations are also defined with scatterglobal and gatherglobal.","category":"page"},{"location":"","page":"MPIHaloArrays.jl","title":"MPIHaloArrays.jl","text":"rank = 0 # MPI rank to scatter from / gather to\n\n# start with a global Base.Array type to decompose and scatter to each rank\nni = 512; nj = 256\nA_global = reshape(1:ni*nj, ni, nj);\n\n# scatter - this internally converts A_global to multiple halo arrays. This is why\n#           the nhalo and topology types are needed\nA_local = scatterglobal(A_global, root, nhalo, topology) # -> returns a MPIHaloArray\n\n# do some work...\n\n# and now gather the decomposed domain and store on the root rank of choice\nA_global_result = gatherglobal(A_local; root=root) # -> returns a Base.Array","category":"page"},{"location":"","page":"MPIHaloArrays.jl","title":"MPIHaloArrays.jl","text":"At the moment, reductions are not implemented, but will be in the future...","category":"page"},{"location":"examples/01-halo1d/#D-Halo-Example","page":"1D Halo Example","title":"1D Halo Example","text":"","category":"section"},{"location":"examples/01-halo1d/","page":"1D Halo Example","title":"1D Halo Example","text":"# examples/01-halo1d.jl\nusing MPI, MPIHaloArrays\n\nMPI.Init()\nconst comm = MPI.COMM_WORLD\nconst rank = MPI.Comm_rank(comm)\nconst nprocs = MPI.Comm_size(comm)\n\n@assert nprocs == 8 \"This example is designed with 8 processes...\"\n\nfunction print_haloarray(A)\n    for p in 0:nprocs-1\n        if rank == p\n            println(\"Rank $(p): $(A)\")\n        end\n        MPI.Barrier(comm)\n    end\nend\n\ntopology = CartesianTopology(comm, 8, false)\n\nnhalo = 2\nni = 8\ndata = collect(1:ni) * (rank + 10)\nA = MPIHaloArray(data, topology, nhalo)\nfillhalo!(A, -1)\n\nif rank == 0 println(\"Before Sync\") end\nprint_haloarray(A)\n\nupdatehalo!(A)\n\nif rank == 0 println(\"\\nAfter Sync\") end\nprint_haloarray(A)\n\nif rank == 0 println(\"Note that the low boundary on 0 and high boundary on 7 are -1 (non-periodic)\") end\nGC.gc()\nMPI.Finalize()","category":"page"},{"location":"examples/01-halo1d/","page":"1D Halo Example","title":"1D Halo Example","text":"> mpiexecjl -n 8 julia examples/01-halo1d.jl\nBefore Sync\nRank 0: [-1, -1, 10, 20, 30, 40, 50, 60, 70, 80, -1, -1]\nRank 1: [-1, -1, 11, 22, 33, 44, 55, 66, 77, 88, -1, -1]\nRank 2: [-1, -1, 12, 24, 36, 48, 60, 72, 84, 96, -1, -1]\nRank 3: [-1, -1, 13, 26, 39, 52, 65, 78, 91, 104, -1, -1]\nRank 4: [-1, -1, 14, 28, 42, 56, 70, 84, 98, 112, -1, -1]\nRank 5: [-1, -1, 15, 30, 45, 60, 75, 90, 105, 120, -1, -1]\nRank 6: [-1, -1, 16, 32, 48, 64, 80, 96, 112, 128, -1, -1]\nRank 7: [-1, -1, 17, 34, 51, 68, 85, 102, 119, 136, -1, -1]\n\nAfter Sync\nRank 0: [-1, -1, 10, 20, 30, 40, 50, 60, 70, 80, 11, 22]\nRank 1: [70, 80, 11, 22, 33, 44, 55, 66, 77, 88, 12, 24]\nRank 2: [77, 88, 12, 24, 36, 48, 60, 72, 84, 96, 13, 26]\nRank 3: [84, 96, 13, 26, 39, 52, 65, 78, 91, 104, 14, 28]\nRank 4: [91, 104, 14, 28, 42, 56, 70, 84, 98, 112, 15, 30]\nRank 5: [98, 112, 15, 30, 45, 60, 75, 90, 105, 120, 16, 32]\nRank 6: [105, 120, 16, 32, 48, 64, 80, 96, 112, 128, 17, 34]\nRank 7: [112, 128, 17, 34, 51, 68, 85, 102, 119, 136, -1, -1]\nNote that the low boundary on 0 and high boundary on 7 are -1 (non-periodic)","category":"page"}]
}
